{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day01_Assignment_Search_Ai_Technology_v1.0.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNee7QOGGpbhv2j8bvoSQFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simon-Neo/AI_Military_Academy/blob/master/Day01_Assignment_Search_Ai_Technology_v1_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL_JggneJ52i",
        "colab_type": "text"
      },
      "source": [
        "### <언어, 음성, 이미지, 자율주행  기술을 사용한 서비스 제품 분석.> \n",
        "Day 01 과제.\n",
        "\n",
        "\n",
        "**1.   자율 주행**\n",
        " \n",
        ">  내가 가장 관심 있는 분야인 컴퓨터 비전과 딥러닝 기술이 접목된 분야라고 할 수 있다. 현재 가장 많이 쓰이는 분야는 영상분석을 통한 사물을 분류해내는 작업에 딥러닝 기계 학습이 유용하다. 무수히 많은 양의 이미지를 입력해서 기계학습을 통해 사람, 자동차, 사물을 분류하는 작업에 쓰인다.\n",
        "![대체 텍스트](https://blog.hmgjournal.com/images_n/contents/deep-learning-carfuture%20(6).jpg)\n",
        "\n",
        ">    <사람의 눈의 역할을 대신 한다고 보면 될것 같다.>\n",
        "\n",
        "> 이미지 분석에서의 기계학습은 픽셀 단위로 영상(이미지를)을 분석하고 확률적인 특징을 찾아내고 분류한다. 예를 들면 사람은 눈, 코, 입이 있으며 위치는 비율적으로 이렇다. 그러면 몇 %는 사람과 가깝다 또는 바퀴가 있고 뒤에 빨간 후미등과 앞뒤에 창문이 있다. 차에 가깝다 이런 식으로 무수히 많은 사람 또는 자동차 영상을 통해서 기계는 차는 이렇다. 사람은 이렇다. 정의하게 된다.\n",
        "\n",
        "* CNN(Convolutional Neural Network)\n",
        "\n",
        ">![대체 텍스트](https://4.bp.blogspot.com/-0JzNdbPz0QI/V04PBfUyl-I/AAAAAAAAyus/lVGB06S-qgMF3aa9sRwiVH0XiRbPh587QCLcB/s400/fig1.png)\n",
        "\n",
        "> 좀더  자세히 이미지 분석을 알아보자면 위 그림과 같이 32 X 32 이미지 정보를 입력값으로 넣는다면 이 데이터를 토대로 특징을 학습하고 정답과 비교하고 틀렸다면 틀린 부분을 최적화하는 하나의 과정을 설계한 게 CNN이다.\n",
        "\n",
        "> 32 x 32의 이미지를 행렬로 가로 행 세로 행으로 이미지의 값을 숫자로 보는 게 CNN의 시작이다. 그다음 특징을 찾아내기 위해서 필터를 사용한다 그 필터로 각 데이터를 곱셈하면서 특징을 좁혀가는 것이다. 필터는 나는 추상적으로 돋보기와 같다고 생각했다 이미지의 상단부터 부분적으로 값을 도출해 보는 것이다. 그리고 가장 큰 값들만을 모아서 다시 이미지를 반으로 줄여서 특징을 응집해 보고 이 단계를 지나서 구해진 새로운 분석된 값들을 정답과 비교해서 학습하는 과정이 CNN의 복잡한 과정이라고 할 수 있겠다.\n",
        "\n",
        ">분석에서 끝난다면 아무 의미가 없을 것이다. 하지만 이렇게 분석된 정보로 옆 차가 깜빡이를 켰다면 속도를 줄인다든지 프로그래밍할 것이다. 중요한 것은 사람의 눈과 사물 인식이 그 시작이라는데 중점을 두고 싶다.\n",
        "\n",
        "<공부해야 할 방향>\n",
        "*   먼저는 딥러닝을 통해서 이미지의 특징을 잡아내고 학습시킬지에 대해서 공부하여야 한다.\n",
        "*   OpenCv 라이브러리를 이용해서 이미지를 어떻게 다루어야 할지 익힌다.\n",
        "*   카메라 뿐 아니라 라이다 센서에 관해서도 공부해야 할 것이다. 3D 좌표의 정보들을 통해서 어떻게 카메라 없이도 오브젝트 디텍션이 일어나게 해야 할지도 공부해야 하고 3D 정보를 2D로 가져오기도 하는 반대의 경우는 어떻게 할지도 공부해야 할 분야이다(렌더링 파이프라인과 관련될지...).\n",
        "공부할 도서 : Open CV 4로 배우는 컴퓨터 비전과 머신 러닝 \n",
        "\n",
        "**2.   이미지**\n",
        "\n",
        "> 자율 주행에서 영상을 다뤘기 때문에 이미지 또한 비슷한 맥락을 이어 갈 것 같다. 영상이 곧 이미지의 나열이기 때문이다. \n",
        "\n",
        "> 하나의 서비스를 소개하려고 한다. 비프로11은 해외 축구 구단에 선수 데이터 정보를 분석하고 통계내주는 서비스를 하고 있다. 각 3개의 카메라만을 설치해서 이미지 정보를 가져오고 이를 사람이 아니라 인공지능이 분석하는 시스템이다. 물론 비프로의 세세한 기술을 알지 못한 단지 내 생각과 추측인점을 명시하고 싶다.\n",
        "![대체 텍스트](https://post-phinf.pstatic.net/MjAyMDAyMjdfMzgg/MDAxNTgyNzk1NzUzNDQx.TPIM1flB7or8JDr2b8M3E8eRSutQYJeWzwNfufXT2p0g.UB9kmc5ewt1cHAVpXLoziLd2hxN-hav1zZv-5-PBIO8g.PNG/analysis4.png?type=w1200)\n",
        "\n",
        "> 그림에 보는 바와 같이 선수, 공, 심판을 명확히 분류하고 있다. 그리고 끊임없이 축구공을 트레킹하면서 축구공의 위치를 실시간으로 기록한다. 여기서 중요한 것은 사람이 직접 공의 위치를 갱신하는 게 아니며 컴퓨터가 이미지의 명암 혹은 컬러를 기준으로 앞뒤 이미지와 비교해가며 계속 추적한다는 것이다. 사람의 눈은 지치겠지만 컴퓨터는 단순 작업에 불과하다. 선수가 공을 잡은 횟수, 패스, 슈팅 모든 걸 기록하고 통계를 내린다. 사람이 기록하던 정보를 인공지능이 대신 수행해준다.\n",
        "\n",
        "> ![대체 텍스트](https://pythonawesome.com/content/images/2019/10/Realtime-Action-Recognition.jpg)\n",
        "\n",
        "> 더 나아가서 슈팅과 패스는 어떻게 구별할지에 대해서도 생각해보면 키를 기준으로 팔의 위치와 허리 다리 위치들이 얼추 비슷한 비율을 유지하기 때문에 인공지능은 어느 곳에 어떤 관절들이 있는지 학습할 것이다. 그리고 실시간으로 선수의 이미지에서 뼈의 위치를 잡아낼 수 있고 슈팅할 때 팔이 얼마나 벌어지고 다리는 어느 위치에 있는지 많은 데이터를 분석하고 학습한다면 패스를 하는 것인지 슛을 하는 것인지 알 수 있을 것이라 추측해 본다.\n",
        "\n",
        "**3.   음성**\n",
        "\n",
        "**4.   언어**\n",
        "\n",
        "> 음성과 언어를 한데 묶어서 인공지능 예약 서비스인 구글 듀플렉스를 소개하고 싶다.\n",
        "![대체 텍스트](https://slownews.kr/wp-content/uploads/2018/05/1595ab88cd0778eaa9e451c22f0af3a1-1.png)\n",
        "> 구글 듀플렉스는 사람의 말을 이해하고 심지어 상점에 전화를 걸어서 사용자가 예약한 내용대로 직접 예약 전화를 점원과 수행했다. 사람인지 기계인지 구별하지 못할 만큼 정교하고 자연스러웠다. 특히 중간에 사람처럼 \"Mm-hmm\" 같은 추임새도 빼놓지 않았다. 놀랍다.\n",
        "\n",
        "*   end-to-end\n",
        "\n",
        ">end-to-end 학습 예를 들어서 음성 인식에서 \"Hey google, play the music.!\" 이라고 주문을 했다고 하자. 기존 음성인식 방식에서는 \n",
        "![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile22.uf.tistory.com%2Fimage%2F99B35E3A5BBFE7E9382ECD)\n",
        "\n",
        ">그림과 같이 오디오 소스 -> MFCC(녹음파일 특징을 추출하는 알고리즘)로 특징 추출 -> 음소 단위('h', 'e', 'y')로 나눔 -> 이걸 다시 조합해 'hey' 단어로 만듦 -> 대본으로 단어 연결하는 복잡한 과정을 통해서 음성을 인식하였다.\n",
        "![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F992A33395BBFEBF91E9BDB)\n",
        "\n",
        ">하지만 end to end 방식은 다량의 데이터를 토대로 기계가 학습하고 결과를 도출하는 것이다. \"hey google\", \"hey Apple\" 여러 단어를 집어넣어서 hey가 하나의 단어로 쓰인다는 걸 인지 시켜 버리는 것과 같다고 나는 생각하였다.\n",
        "\n",
        "> end to end 방식은 기존에 방식의 알고리즘과 음소 단위의 분석에 관한 여러 복잡한 과정을 뛰어넘고 고속도로를 달리듯 바로 학습데이터를 기반으로 결과를 알려주는 획기적인 기술이다.\n",
        "\n",
        "> end to end의 단점은 대량의 데이터가 필요하다는 것이다. 소량의 데이터는 학습에 효과적이지 못해 기존의 방식보다 정확도가 떨어질 것이다. 하지만 대량의 데이터 학습은 오류 확률을 줄일 것이며 더욱 정확하고 자연스러운 음성인식 변화를 가져올 것이다.\n",
        "\n",
        "*   RNN(Recurrent Neural Network)\n",
        "\n",
        ">구글에서 듀플렉스를 만드는 데 있어서 가장 핵심적인 기술은 RNN 신경망 기술이었다고 한다. RNN이 왜 중요한 기술이었는지 살펴보고자 한다.\n",
        "\n",
        ">\"I had to bear of bear's power\" 말도 안 되는 문장이지만 이런 말이 있다고 하자 나는 곰의 힘을 견뎌야 해야만 했다는 문장이 있다면 과연 일반 딥러닝 알고리즘은 동사 bear와 명사 bear를 구분 할 수 있을까? 문맥에 따라서 판단하는 데 한계가 있으리라 생각한다. 한마디로 앞뒤 문장이 또는 문맥상에 따라 동사가 되기도 명사가 되기도 하는 문장에 대처 하야만 한다. \n",
        "\n",
        "\n",
        ">![대체텍스트](https://lh3.googleusercontent.com/proxy/YE73Nl06RmiADvfB3V6Dx1ZGZU8EfpId6osBC1_wi766o1STA1XiSzu8-CdRjlwaktDn92dPuIl3ThA8dm773brlc_bi1SpOdmaj2Sp5ju4)\n",
        "\n",
        "> Recurrent 되풀이된다는 뜻인데 내가 이해하는 건 재귀라고 이해하기로 했다. 현재의 은닉층 결과가 누적적으로 다음 가중치에 영향을 준다. 고로 had to  다음에는 동사가 나온다는 걸 RNN 신경망을 통해서 학습할 수 있다.  쉽게 말해 앞의 문장들이 현재 문장에 영향을 준다. 고로 문맥을 이해하는 방식을 설명한다.\n",
        "\n",
        "> 구글은 최대한 많은 실제 전화 녹음 파일을 RNN으로 학습시켰으며 손실 함수(loss function)를 학습을 통해 줄여나가는 식으로 사람과 동등한 수준의 사람의 말을 이해하고 문자를 만들어서 대화하는 AI를 개발했다.\n",
        "\n",
        "> 내가 느낀 점은 앞으로 콜센터나 아나운서와 같은 서비스에 적극적으로 언어와 음성 인식들이 사람을 대처할 것으로 보인다고 하지만 명과 암은 존재할 것 같아 걱정이 앞선다. AI를 이용한 사기 범죄가 기승을 부릴 것이고 이게 AI인지 사람인지 전화를 하면서도 의심하는 시대가 되지 않을까 조심스럽게 예측해본다.\n",
        "\n",
        "> RNN을 원리뿐 아니라 직접 구현해보면서 오디오 소스를 AI가 인지하도록 만들어보는 게 학습 목표가 아닐까 싶다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BksUhPgyrKBq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}